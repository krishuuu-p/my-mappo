"""
MA-LSTM Policy for MAPPO.

This module implements an LSTM actor and centralized critic for the MA-LSTM-PPO
algorithm, compatible with the existing MAPPO codebase.

Reference: docs/MA-LSTM-PPO-paper-summary.md Section 2 (Model architecture)

Actor (per-agent, recurrent):
- Input: agent observation vector o_i (obs_dim)
- Dense -> ReLU -> LSTM (hidden H) -> Dense -> action mean mu (action_dim)
- Learnable log_std parameter (diagonal Gaussian)
- Output: continuous v_des = [v_x, v_y, v_z, thrust]
- Optional tanh to bound outputs

Centralized Critic:
- Input: share_obs (global state: concatenated positions, velocities of all agents)
- Dense -> ReLU -> (optional LSTM) -> Dense -> scalar value V(s)

I/O contract (must match MAPPO runner):
- act(obs, share_obs, rnn_states, deterministic) -> (value, action, log_probs, rnn_states_out)
- evaluate_actions(obs, share_obs, actions, rnn_states) -> (values, log_probs, entropy)

Author: MA-LSTM-PPO Integration
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Tuple, Optional, Union

from onpolicy.algorithms.utils.util import init, check
from onpolicy.utils.util import get_shape_from_obs_space


def init_weights(module, gain=np.sqrt(2), bias=0.0):
    """Initialize weights with orthogonal initialization."""
    if isinstance(module, (nn.Linear, nn.Conv2d)):
        nn.init.orthogonal_(module.weight, gain=gain)
        if module.bias is not None:
            nn.init.constant_(module.bias, bias)
    return module


class LSTMActor(nn.Module):
    """
    LSTM-based Actor network for MA-LSTM-PPO.
    
    Reference: docs/MA-LSTM-PPO-paper-summary.md Section 2
    - Input: agent observation vector o_i (obs_dim)
    - Dense -> ReLU -> LSTM (hidden H) -> Dense -> action mean mu (action_dim)
    - Learnable log_std parameter (diagonal Gaussian)
    """
    
    def __init__(
        self,
        obs_dim: int,
        action_dim: int,
        hidden_size: int = 64,
        lstm_hidden_size: int = 64,
        num_lstm_layers: int = 1,
        use_orthogonal: bool = True,
        use_tanh_output: bool = True,
        action_scale: float = 1.0,
        log_std_init: float = 0.0,
        log_std_min: float = -20.0,
        log_std_max: float = 2.0,
        device: torch.device = torch.device("cpu"),
    ):
        """
        Initialize LSTM Actor.
        
        Args:
            obs_dim: Observation dimension
            action_dim: Action dimension (4 for v_des = [vx, vy, vz, throttle])
            hidden_size: Hidden layer size for MLP
            lstm_hidden_size: LSTM hidden state size
            num_lstm_layers: Number of LSTM layers
            use_orthogonal: Whether to use orthogonal initialization
            use_tanh_output: Whether to apply tanh to bound actions
            action_scale: Scale factor for actions after tanh
            log_std_init: Initial value for log standard deviation
            log_std_min: Minimum log standard deviation
            log_std_max: Maximum log standard deviation
            device: Torch device
        """
        super(LSTMActor, self).__init__()
        
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.hidden_size = hidden_size
        self.lstm_hidden_size = lstm_hidden_size
        self.num_lstm_layers = num_lstm_layers
        self.use_tanh_output = use_tanh_output
        self.action_scale = action_scale
        self.log_std_min = log_std_min
        self.log_std_max = log_std_max
        
        self.tpdv = dict(dtype=torch.float32, device=device)
        
        # Input feature extraction: Dense -> ReLU
        # Reference: docs/MA-LSTM-PPO-paper-summary.md Section 2
        init_method = nn.init.orthogonal_ if use_orthogonal else nn.init.xavier_uniform_
        
        self.fc1 = nn.Sequential(
            nn.Linear(obs_dim, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
        )
        
        # LSTM layer
        # Reference: docs/MA-LSTM-PPO-paper-summary.md Section 2
        self.lstm = nn.LSTM(
            input_size=hidden_size,
            hidden_size=lstm_hidden_size,
            num_layers=num_lstm_layers,
            batch_first=True,
        )
        
        # Output layer: action mean
        self.action_mean = nn.Linear(lstm_hidden_size, action_dim)
        
        # Learnable log_std parameter
        # Reference: docs/MA-LSTM-PPO-paper-summary.md Section 2
        self.log_std = nn.Parameter(torch.ones(action_dim) * log_std_init)
        
        # Initialize weights
        self._init_weights(use_orthogonal)
        
        self.to(device)
    
    def _init_weights(self, use_orthogonal: bool):
        """Initialize network weights."""
        gain = np.sqrt(2)
        for module in self.fc1:
            if isinstance(module, nn.Linear):
                if use_orthogonal:
                    nn.init.orthogonal_(module.weight, gain)
                else:
                    nn.init.xavier_uniform_(module.weight)
                nn.init.constant_(module.bias, 0)
        
        # LSTM initialization
        for name, param in self.lstm.named_parameters():
            if 'weight' in name:
                nn.init.orthogonal_(param) if use_orthogonal else nn.init.xavier_uniform_(param)
            elif 'bias' in name:
                nn.init.constant_(param, 0)
        
        # Action mean with smaller gain for stability
        if use_orthogonal:
            nn.init.orthogonal_(self.action_mean.weight, 0.01)
        else:
            nn.init.xavier_uniform_(self.action_mean.weight)
        nn.init.constant_(self.action_mean.bias, 0)
    
    def forward(
        self,
        obs: torch.Tensor,
        rnn_states: torch.Tensor,
        masks: torch.Tensor,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Forward pass through actor network.
        
        Args:
            obs: Observations (batch, obs_dim)
            rnn_states: LSTM hidden states (2, num_layers, batch, lstm_hidden_size)
            masks: Mask for resetting RNN states (batch, 1)
            
        Returns:
            action_mean: Mean of action distribution (batch, action_dim)
            action_std: Std of action distribution (batch, action_dim)
            rnn_states_out: Updated LSTM hidden states
        """
        batch_size = obs.shape[0]
        
        # Debug input shapes (first time only)
        if not hasattr(self, '_debug_printed'):
            print(f"[DEBUG] forward input shapes:")
            print(f"  obs: {obs.shape}")
            print(f"  rnn_states: {rnn_states.shape}")
            print(f"  masks: {masks.shape}")
            print(f"  Expected: rnn_states=(batch, num_layers, hidden)")
            print(f"  Will split into h and c")
            self._debug_printed = True
        
        # Feature extraction
        features = self.fc1(obs)  # (batch, hidden_size)
        
        # Prepare LSTM input (batch, seq_len=1, hidden_size)
        lstm_input = features.unsqueeze(1)
        
        # Handle RNN states: buffer stores (batch, recurrent_N=2, hidden_size)
        # recurrent_N=2: index 0 = h (hidden state), index 1 = c (cell state)
        n_layers = self.num_lstm_layers  # actual LSTM layers (1)
        h_state = rnn_states[:, :n_layers, :]   # (batch, 1, hidden)
        c_state = rnn_states[:, n_layers:, :]   # (batch, 1, hidden)
        
        # Transpose to LSTM format: (num_layers, batch, hidden)
        h = h_state.transpose(0, 1)  # (1, batch, hidden)
        c = c_state.transpose(0, 1)  # (1, batch, hidden)
        
        # Handle masks (reset hidden states where mask is 0)
        # masks shape: (batch, 1) -> need (1, batch, 1) for broadcasting
        mask_for_broadcast = masks.unsqueeze(0)  # (1, batch, 1)
        h = h * mask_for_broadcast
        c = c * mask_for_broadcast
        
        # LSTM forward
        lstm_out, (h_new, c_new) = self.lstm(lstm_input, (h, c))
        
        # Output
        lstm_out = lstm_out.squeeze(1)  # (batch, lstm_hidden_size)
        action_mean = self.action_mean(lstm_out)  # (batch, action_dim)
        
        # Apply tanh for bounded actions
        if self.use_tanh_output:
            action_mean = torch.tanh(action_mean) * self.action_scale
        
        # Get std from learnable log_std
        log_std = torch.clamp(self.log_std, self.log_std_min, self.log_std_max)
        action_std = torch.exp(log_std).expand_as(action_mean)
        
        # Return hidden states in buffer format: (batch, recurrent_N=2, hidden)
        # Concatenate h_new and c_new: h at index 0, c at index 1
        rnn_states_out = torch.cat([h_new, c_new], dim=0).transpose(0, 1)  # (batch, 2, hidden)
        
        return action_mean, action_std, rnn_states_out
    
    def sample(
        self,
        obs: torch.Tensor,
        rnn_states: torch.Tensor,
        masks: torch.Tensor,
        deterministic: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Sample actions from the policy.
        
        Args:
            obs: Observations
            rnn_states: LSTM hidden states
            masks: Mask for resetting RNN states
            deterministic: If True, return mean action
            
        Returns:
            actions: Sampled actions
            log_probs: Log probabilities of actions
            rnn_states_out: Updated hidden states
        """
        action_mean, action_std, rnn_states_out = self.forward(obs, rnn_states, masks)
        
        if deterministic:
            actions = action_mean
            # Compute log prob of mean action
            dist = torch.distributions.Normal(action_mean, action_std)
            log_probs = dist.log_prob(actions).sum(dim=-1, keepdim=True)
        else:
            # Sample from Gaussian
            dist = torch.distributions.Normal(action_mean, action_std)
            actions = dist.sample()
            log_probs = dist.log_prob(actions).sum(dim=-1, keepdim=True)
        
        return actions, log_probs, rnn_states_out
    
    def evaluate(
        self,
        obs: torch.Tensor,
        actions: torch.Tensor,
        rnn_states: torch.Tensor,
        masks: torch.Tensor,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Evaluate log probability and entropy of given actions.
        
        Args:
            obs: Observations
            actions: Actions to evaluate
            rnn_states: LSTM hidden states
            masks: Mask for resetting RNN states
            
        Returns:
            log_probs: Log probabilities of actions
            entropy: Entropy of action distribution
        """
        action_mean, action_std, _ = self.forward(obs, rnn_states, masks)
        
        dist = torch.distributions.Normal(action_mean, action_std)
        log_probs = dist.log_prob(actions).sum(dim=-1, keepdim=True)
        entropy = dist.entropy().sum(dim=-1).mean()
        
        return log_probs, entropy


class CentralizedCritic(nn.Module):
    """
    Centralized Critic network for MA-LSTM-PPO.
    
    Reference: docs/MA-LSTM-PPO-paper-summary.md Section 2
    - Input: share_obs (global state: concatenated positions, velocities of all agents)
    - Dense -> ReLU -> (optional LSTM) -> Dense -> scalar value V(s)
    """
    
    def __init__(
        self,
        share_obs_dim: int,
        hidden_size: int = 64,
        lstm_hidden_size: int = 64,
        num_lstm_layers: int = 1,
        use_lstm: bool = True,
        use_orthogonal: bool = True,
        device: torch.device = torch.device("cpu"),
    ):
        """
        Initialize Centralized Critic.
        
        Args:
            share_obs_dim: Shared observation dimension
            hidden_size: Hidden layer size for MLP
            lstm_hidden_size: LSTM hidden state size
            num_lstm_layers: Number of LSTM layers
            use_lstm: Whether to use LSTM in critic
            use_orthogonal: Whether to use orthogonal initialization
            device: Torch device
        """
        super(CentralizedCritic, self).__init__()
        
        self.share_obs_dim = share_obs_dim
        self.hidden_size = hidden_size
        self.lstm_hidden_size = lstm_hidden_size
        self.num_lstm_layers = num_lstm_layers
        self.use_lstm = use_lstm
        
        self.tpdv = dict(dtype=torch.float32, device=device)
        
        # Feature extraction: Dense -> ReLU
        self.fc1 = nn.Sequential(
            nn.Linear(share_obs_dim, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
        )
        
        # Optional LSTM layer
        if use_lstm:
            self.lstm = nn.LSTM(
                input_size=hidden_size,
                hidden_size=lstm_hidden_size,
                num_layers=num_lstm_layers,
                batch_first=True,
            )
            value_input_size = lstm_hidden_size
        else:
            self.lstm = None
            value_input_size = hidden_size
        
        # Value output
        self.value_out = nn.Linear(value_input_size, 1)
        
        # Initialize weights
        self._init_weights(use_orthogonal)
        
        self.to(device)
    
    def _init_weights(self, use_orthogonal: bool):
        """Initialize network weights."""
        gain = np.sqrt(2)
        for module in self.fc1:
            if isinstance(module, nn.Linear):
                if use_orthogonal:
                    nn.init.orthogonal_(module.weight, gain)
                else:
                    nn.init.xavier_uniform_(module.weight)
                nn.init.constant_(module.bias, 0)
        
        if self.lstm is not None:
            for name, param in self.lstm.named_parameters():
                if 'weight' in name:
                    nn.init.orthogonal_(param) if use_orthogonal else nn.init.xavier_uniform_(param)
                elif 'bias' in name:
                    nn.init.constant_(param, 0)
        
        if use_orthogonal:
            nn.init.orthogonal_(self.value_out.weight, 1.0)
        else:
            nn.init.xavier_uniform_(self.value_out.weight)
        nn.init.constant_(self.value_out.bias, 0)
    
    def forward(
        self,
        share_obs: torch.Tensor,
        rnn_states: Optional[torch.Tensor] = None,
        masks: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        Forward pass through critic network.
        
        Args:
            share_obs: Shared observations (batch, share_obs_dim)
            rnn_states: LSTM hidden states (optional)
            masks: Mask for resetting RNN states (optional)
            
        Returns:
            values: State values (batch, 1)
            rnn_states_out: Updated LSTM hidden states (or None if not using LSTM)
        """
        # Feature extraction
        features = self.fc1(share_obs)  # (batch, hidden_size)
        
        if self.use_lstm and self.lstm is not None:
            # Prepare LSTM input
            lstm_input = features.unsqueeze(1)  # (batch, 1, hidden_size)
            
            if rnn_states is not None and masks is not None:
                # Buffer format: (batch, recurrent_N=2, hidden)
                # index 0 = h (hidden state), index 1 = c (cell state)
                n_layers = self.num_lstm_layers  # actual LSTM layers (1)
                h_state = rnn_states[:, :n_layers, :]   # (batch, 1, hidden)
                c_state = rnn_states[:, n_layers:, :]   # (batch, 1, hidden)
                h = h_state.transpose(0, 1)  # (1, batch, hidden)
                c = c_state.transpose(0, 1)  # (1, batch, hidden)
                mask_for_broadcast = masks.unsqueeze(0)  # (1, batch, 1)
                h = h * mask_for_broadcast
                c = c * mask_for_broadcast
            else:
                batch_size = share_obs.shape[0]
                h = torch.zeros(self.num_lstm_layers, batch_size, self.lstm_hidden_size).to(**self.tpdv)
                c = torch.zeros(self.num_lstm_layers, batch_size, self.lstm_hidden_size).to(**self.tpdv)
            
            lstm_out, (h_new, c_new) = self.lstm(lstm_input, (h, c))
            lstm_out = lstm_out.squeeze(1)
            # Return in buffer format: (batch, recurrent_N=2, hidden)
            rnn_states_out = torch.cat([h_new, c_new], dim=0).transpose(0, 1)
            
            values = self.value_out(lstm_out)
        else:
            values = self.value_out(features)
            rnn_states_out = None
        
        return values, rnn_states_out


class MA_LSTM_Policy(nn.Module):
    """
    MA-LSTM Policy combining LSTM Actor and Centralized Critic.
    
    This class provides the MAPPO-compatible interface for training and inference.
    
    Reference: docs/MA-LSTM-PPO-paper-summary.md Section 2 (I/O contract)
    - act(obs, share_obs, rnn_states, deterministic) -> (value, action, log_probs, rnn_states_out)
    - evaluate_actions(obs, share_obs, actions, rnn_states) -> (values, log_probs, entropy)
    """
    
    def __init__(
        self,
        args,
        obs_space,
        share_obs_space,
        act_space,
        device: torch.device = torch.device("cpu"),
    ):
        """
        Initialize MA-LSTM Policy.
        
        Args:
            args: Arguments namespace with configuration
            obs_space: Observation space
            share_obs_space: Shared observation space (for centralized critic)
            act_space: Action space
            device: Torch device
        """
        super(MA_LSTM_Policy, self).__init__()
        
        self.device = device
        self.tpdv = dict(dtype=torch.float32, device=device)
        
        # Get dimensions from spaces
        self.obs_dim = get_shape_from_obs_space(obs_space)[0]
        self.share_obs_dim = get_shape_from_obs_space(share_obs_space)[0]
        
        # Action space handling
        if hasattr(act_space, 'shape'):
            self.action_dim = act_space.shape[0]
        elif hasattr(act_space, 'n'):
            self.action_dim = act_space.n
        else:
            self.action_dim = 4  # Default for v_des
        
        # Get hyperparameters from args
        hidden_size = getattr(args, 'hidden_size', 64)
        use_orthogonal = getattr(args, 'use_orthogonal', True)
        recurrent_N = getattr(args, 'recurrent_N', 1)
        
        # Learning rates
        self.lr = getattr(args, 'lr', 5e-4)
        self.critic_lr = getattr(args, 'critic_lr', 5e-4)
        self.opti_eps = getattr(args, 'opti_eps', 1e-5)
        self.weight_decay = getattr(args, 'weight_decay', 0)
        
        # Create actor network
        # Reference: docs/MA-LSTM-PPO-paper-summary.md Section 2 (Actor)
        # Note: num_lstm_layers=1 always; recurrent_N=2 is only for buffer (stores h and c)
        self.actor = LSTMActor(
            obs_dim=self.obs_dim,
            action_dim=self.action_dim,
            hidden_size=hidden_size,
            lstm_hidden_size=hidden_size,
            num_lstm_layers=1,
            use_orthogonal=use_orthogonal,
            use_tanh_output=True,
            device=device,
        )
        
        # Create critic network
        # Reference: docs/MA-LSTM-PPO-paper-summary.md Section 2 (Centralized Critic)
        self.critic = CentralizedCritic(
            share_obs_dim=self.share_obs_dim,
            hidden_size=hidden_size,
            lstm_hidden_size=hidden_size,
            num_lstm_layers=1,
            use_lstm=True,
            use_orthogonal=use_orthogonal,
            device=device,
        )
        
        # Optimizers
        self.actor_optimizer = torch.optim.Adam(
            self.actor.parameters(),
            lr=self.lr,
            eps=self.opti_eps,
            weight_decay=self.weight_decay,
        )
        self.critic_optimizer = torch.optim.Adam(
            self.critic.parameters(),
            lr=self.critic_lr,
            eps=self.opti_eps,
            weight_decay=self.weight_decay,
        )
        
        # Store args for compatibility
        self._hidden_size = hidden_size
        self._recurrent_N = recurrent_N
    
    def get_actions(
        self,
        cent_obs: np.ndarray,
        obs: np.ndarray,
        rnn_states_actor: np.ndarray,
        rnn_states_critic: np.ndarray,
        masks: np.ndarray,
        available_actions: np.ndarray = None,
        deterministic: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Compute actions and value predictions.
        
        Reference: docs/MA-LSTM-PPO-paper-summary.md Section 2 (I/O contract)
        
        Args:
            cent_obs: Centralized observations for critic (batch, share_obs_dim)
            obs: Local observations for actor (batch, obs_dim)
            rnn_states_actor: Actor RNN states
            rnn_states_critic: Critic RNN states
            masks: RNN masks
            available_actions: Available actions mask (unused for continuous)
            deterministic: Whether to use deterministic actions
            
        Returns:
            values: Value predictions
            actions: Sampled actions
            action_log_probs: Log probabilities of actions
            rnn_states_actor: Updated actor RNN states
            rnn_states_critic: Updated critic RNN states
        """
        # Convert inputs to tensors
        obs = check(obs).to(**self.tpdv)
        cent_obs = check(cent_obs).to(**self.tpdv)
        rnn_states_actor = check(rnn_states_actor).to(**self.tpdv)
        rnn_states_critic = check(rnn_states_critic).to(**self.tpdv)
        masks = check(masks).to(**self.tpdv)
        
        # Get actions from actor
        actions, action_log_probs, rnn_states_actor_new = self.actor.sample(
            obs, rnn_states_actor, masks, deterministic
        )
        
        # Get values from critic
        values, rnn_states_critic_new = self.critic(cent_obs, rnn_states_critic, masks)
        
        return values, actions, action_log_probs, rnn_states_actor_new, rnn_states_critic_new
    
    def get_values(
        self,
        cent_obs: np.ndarray,
        rnn_states_critic: np.ndarray,
        masks: np.ndarray,
    ) -> torch.Tensor:
        """
        Get value predictions.
        
        Args:
            cent_obs: Centralized observations
            rnn_states_critic: Critic RNN states
            masks: RNN masks
            
        Returns:
            values: Value predictions
        """
        cent_obs = check(cent_obs).to(**self.tpdv)
        rnn_states_critic = check(rnn_states_critic).to(**self.tpdv)
        masks = check(masks).to(**self.tpdv)
        
        values, _ = self.critic(cent_obs, rnn_states_critic, masks)
        return values
    
    def evaluate_actions(
        self,
        cent_obs: np.ndarray,
        obs: np.ndarray,
        rnn_states_actor: np.ndarray,
        rnn_states_critic: np.ndarray,
        action: np.ndarray,
        masks: np.ndarray,
        available_actions: np.ndarray = None,
        active_masks: np.ndarray = None,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Evaluate actions for PPO update.
        
        Reference: docs/MA-LSTM-PPO-paper-summary.md Section 2 (I/O contract)
        
        Args:
            cent_obs: Centralized observations
            obs: Local observations
            rnn_states_actor: Actor RNN states
            rnn_states_critic: Critic RNN states
            action: Actions to evaluate
            masks: RNN masks
            available_actions: Available actions mask (unused)
            active_masks: Active agent masks (unused)
            
        Returns:
            values: Value predictions
            action_log_probs: Log probabilities of given actions
            dist_entropy: Distribution entropy
        """
        # Convert inputs to tensors
        obs = check(obs).to(**self.tpdv)
        cent_obs = check(cent_obs).to(**self.tpdv)
        rnn_states_actor = check(rnn_states_actor).to(**self.tpdv)
        rnn_states_critic = check(rnn_states_critic).to(**self.tpdv)
        action = check(action).to(**self.tpdv)
        masks = check(masks).to(**self.tpdv)
        
        # Evaluate actions with actor
        action_log_probs, dist_entropy = self.actor.evaluate(
            obs, action, rnn_states_actor, masks
        )
        
        # Get values from critic
        values, _ = self.critic(cent_obs, rnn_states_critic, masks)
        
        return values, action_log_probs, dist_entropy
    
    def act(
        self,
        obs: np.ndarray,
        rnn_states_actor: np.ndarray,
        masks: np.ndarray,
        available_actions: np.ndarray = None,
        deterministic: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Compute actions for execution (no value computation).
        
        Args:
            obs: Local observations
            rnn_states_actor: Actor RNN states
            masks: RNN masks
            available_actions: Available actions mask (unused)
            deterministic: Whether to use deterministic actions
            
        Returns:
            actions: Sampled actions
            rnn_states_actor: Updated RNN states
        """
        obs = check(obs).to(**self.tpdv)
        rnn_states_actor = check(rnn_states_actor).to(**self.tpdv)
        masks = check(masks).to(**self.tpdv)
        
        actions, _, rnn_states_actor_new = self.actor.sample(
            obs, rnn_states_actor, masks, deterministic
        )
        
        return actions, rnn_states_actor_new
    
    def lr_decay(self, episode: int, episodes: int):
        """Decay learning rates linearly."""
        from onpolicy.utils.util import update_linear_schedule
        update_linear_schedule(self.actor_optimizer, episode, episodes, self.lr)
        update_linear_schedule(self.critic_optimizer, episode, episodes, self.critic_lr)


def get_ma_lstm_policy_class():
    """Return the MA_LSTM_Policy class for factory use."""
    return MA_LSTM_Policy
